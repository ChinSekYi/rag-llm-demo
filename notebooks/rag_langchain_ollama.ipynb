{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f11eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader, DirectoryLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/pdf\",\n",
    "    glob=\"**/*.pdf\",  \n",
    "    loader_cls=PyMuPDFLoader,\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "pdf_documents = dir_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a301af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    return split_docs\n",
    "\n",
    "all_splits=split_documents(pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce6d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"../chroma_db\"  # Saves to project root\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3358a8f5",
   "metadata": {},
   "source": [
    "## RAG Retriever & LLM Integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44a4163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 documents for: 'What is deep learning?'\n"
     ]
    }
   ],
   "source": [
    "## Create a retriever from the vector store\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # Return top 3 most similar documents\n",
    ")# List[Document]\n",
    "\n",
    "# Test the retriever\n",
    "query = \"What is deep learning?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents for: '{query}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf4716a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama LLM initialized successfully\n"
     ]
    }
   ],
   "source": [
    "## Set up LLM\n",
    "# Using Ollama (local LLM)\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "try:\n",
    "    llm = ChatOllama(\n",
    "        model=\"gpt-oss:20b\",  # Change to your installed model (llama3, mistral, etc.)\n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(\"Ollama LLM initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Ollama initialization failed: {e}\")\n",
    "    print(\"Make sure Ollama is running: ollama serve\")\n",
    "    llm = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b7278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain created successfully\n"
     ]
    }
   ],
   "source": [
    "## Create RAG Chain\n",
    "# Define the system prompt\n",
    "system_prompt = \"\"\"You are an AI assistant that answers questions based on the provided context. \n",
    "If the context doesn't contain the answer, say \"I don't have enough information to answer this question.\"\n",
    "Keep your answers concise and relevant.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"Context:\\n{context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "# Create RAG chain manually\n",
    "if llm:\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    print(\"RAG chain created successfully\")\n",
    "else:\n",
    "    rag_chain = None\n",
    "    print(\"RAG chain not created (LLM not available)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10489987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is deep learning?\n",
      "\n",
      "Answer:\n",
      "I don't have enough information to answer this question.\n",
      "\n",
      "Retrieved Sources (3 docs):\n",
      "  1. ../data/pdf/nlp_llm/Language-Models-are-Few-Shot-Learners.pdf\n",
      "  2. ../data/pdf/nlp_llm/Language-Models-are-Few-Shot-Learners.pdf\n",
      "  3. ../data/pdf/nlp_llm/Language-Models-are-Few-Shot-Learners.pdf\n",
      "============================================================\n",
      "Query: Explain attention mechanisms\n",
      "\n",
      "Answer:\n",
      "Attention mechanisms let a model “look at” different parts of an input sequence when producing each output token.  \n",
      "The core idea is to compute, for each query vector **q**, a weighted sum of value vectors **v** where the weights come from a similarity score between **q** and each key vector **k**.\n",
      "\n",
      "**Key steps**\n",
      "\n",
      "1. **Linear projections**  \n",
      "   The input embeddings are projected into three spaces (queries, keys, values) using learned weight matrices. In multi‑head attention this is done *h* times with different projections, producing \\(h\\) sets of \\((q, k, v)\\) each of size \\(d_k, d_k, d_v\\).\n",
      "\n",
      "2. **Similarity scores**  \n",
      "   For one head the dot product \\(q \\cdot k\\) is computed. Because the components of \\(q\\) and \\(k\\) are roughly unit‑variance, the dot product’s variance grows with the dimension \\(d_k\\). To keep the softmax gradients stable, the scores are divided by \\(\\sqrt{d_k}\\).\n",
      "\n",
      "3. **Softmax weighting**  \n",
      "   The scaled scores are passed through a softmax to obtain attention weights \\(\\alpha_i\\) that sum to 1. This turns the similarity into a probability distribution over positions.\n",
      "\n",
      "4. **Weighted sum of values**  \n",
      "   Each value vector is multiplied by its weight and summed:  \n",
      "   \\[\n",
      "   \\text{output} = \\sum_i \\alpha_i\\, v_i\n",
      "   \\]\n",
      "   This gives a context‑aware representation for the query position.\n",
      "\n",
      "5. **Multi‑head aggregation**  \n",
      "   The outputs from all heads (each capturing different representation subspaces) are concatenated and projected once more to form the final output. This allows the model to attend to diverse aspects of the input simultaneously.\n",
      "\n",
      "**Why it works**\n",
      "\n",
      "- The dot‑product attention focuses computation on the most relevant tokens, improving efficiency over full‑matrix comparisons.\n",
      "- Scaling by \\(\\sqrt{d_k}\\) prevents large dot products from pushing the softmax into saturated regions.\n",
      "- Multi‑head attention lets the model capture multiple types of relationships (e.g., syntax vs. semantics) in parallel.\n",
      "\n",
      "In short, attention mechanisms compute context‑dependent weighted sums of value vectors, guided by learned query‑key similarities, and multi‑head designs extend this to capture richer patterns.\n",
      "\n",
      "Retrieved Sources (3 docs):\n",
      "  1. ../data/pdf/deep_learning/attention-is-all-you-need.pdf\n",
      "  2. ../data/pdf/deep_learning/attention-is-all-you-need.pdf\n",
      "  3. ../data/pdf/deep_learning/attention-is-all-you-need.pdf\n",
      "============================================================\n",
      "Query: What are transformers?\n",
      "\n",
      "Answer:\n",
      "I don't have enough information to answer this question.\n",
      "\n",
      "Retrieved Sources (3 docs):\n",
      "  1. ../data/pdf/nlp_llm/Language-Models-are-Few-Shot-Learners.pdf\n",
      "  2. ../data/pdf/nlp_llm/Language-Models-are-Few-Shot-Learners.pdf\n",
      "  3. ../data/pdf/nlp_llm/Language-Models-are-Few-Shot-Learners.pdf\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "if rag_chain:\n",
    "    # Example queries\n",
    "    test_queries = [\n",
    "        \"What is deep learning?\",\n",
    "        \"Explain attention mechanisms\",\n",
    "        \"What are transformers?\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"Query: {query}\")\n",
    "        \n",
    "        answer = rag_chain.invoke(query)\n",
    "        print(f\"\\nAnswer:\\n{answer}\")\n",
    "        \n",
    "        # Show retrieved sources\n",
    "        retrieved = retriever.invoke(query)\n",
    "        print(f\"\\nRetrieved Sources ({len(retrieved)} docs):\")\n",
    "        for i, doc in enumerate(retrieved, 1):\n",
    "            print(f\"  {i}. {doc.metadata.get('source', 'Unknown')}\")\n",
    "\n",
    "        print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(\"Cannot test RAG chain (LLM not available)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
